 

### Related Work

Code review processes have evolved significantly, with research focusing on assessing review quality and leveraging large language models (LLMs) for automation and enhancement. This section surveys key works in these areas, highlighting how our iterative refinement approach builds on existing metrics and generation techniques while addressing gaps in human-AI collaboration.

#### Quality Assessment Literature
Early efforts in evaluating code review quality emphasized empirical factors and machine learning classifiers to predict usefulness. For instance, Rahman et al. (2018) introduced RevHelper, a Random Forest-based model that predicts the usefulness of review comments using textual features like length and sentiment, alongside developer experience metrics. Achieving around 66% accuracy on datasets from open-source projects, this work underscores the importance of actionability in reviews, which aligns with our use of feedback to identify deficiencies. Similarly, Turzo and Bosu (2023) conducted an empirical investigation into what makes reviews useful in OpenDev projects, analyzing specificity and coverage across ~5,000 reviews. Their findings reveal common gaps in human reviews, such as incompleteness, motivating our threshold-based iteration to enhance coverage.

More recent advancements introduce sophisticated metrics for automated assessment. Naik et al. (2024) proposed CRScore, a reference-free metric that evaluates reviews on comprehensiveness, conciseness, and relevance by generating pseudo-references from LLMs and static analysis tools. With a Spearman correlation of ~0.54 to human judgments, CRScore outperforms traditional metrics like BLEU and forms the core of our pipeline for providing detailed, grounded feedback. Building on this, Naik (2025) extended it to CRScore++ with reinforcement learning for verifiable rewards, enabling cross-language generalization and inspiring potential optimizations in our LLM refinement loop.

Other works critique evaluation frameworks and explore deep learning alternatives. A 2025 Springer paper revisits metrics for code review comment generators, proposing DeepCRCEval with human/LLM evaluators focusing on actionability, which exposes limitations in benchmarks and supports our hybrid evaluation. Cihan (2025) evaluated LLMs like GPT-4o for code correctness detection, achieving ~68% accuracy and advocating human-in-the-loop processes to mitigate errors—echoing our preservation of human intent. Bacchelli et al. (2023) employed BERT for multi-label classification of review attributes (e.g., suggestions, emotions), attaining F1 scores of 0.82–0.94 and outperforming traditional models, which we could adapt for pre-filtering in future extensions. Additional reviews, such as a 2024/2025 MDPI paper on quality metrics and a 2025 ResearchGate critical review of benchmarks, highlight scalability issues in scoring, reinforcing CRScore's role in our systematic refinements.

#### Review Generation and Enhancement with LLMs
Automated review generation has progressed from pre-trained models to LLM-driven enhancements. Li et al. (2022) developed CodeReviewer, a large-scale pre-trained model for predicting comments from diffs across nine languages, using datasets like 20,888 Python samples. As a baseline in our work, it demonstrates noisy human data that benefits from iterative improvement. Andersson and Nelson (2025) introduced ACR, an agentic framework with LLMs and knowledge graphs for context-aware reviews in enterprise settings like Ericsson, reducing cognitive load and aligning with our scalability goals for software teams.

Enhancement techniques increasingly incorporate feedback loops. A 2024 ACM paper on ClarifyGPT uses LLMs to generate clarification questions for ambiguous requirements, improving code generation reliability and paralleling our use of CRScore feedback to address review gaps. A 2024 journal paper on AICodeReview presents an IntelliJ plugin with GPT-3.5 for syntax and semantics suggestions, streamlining reviews but lacking iteration—our approach adds this for better alignment with human insights. Surveys like a 2025 arXiv paper on LLM agents for code generation discuss multi-agent collaboration, positioning our method as a novel application in refinement. Finally, a 2025 ResearchGate evaluation of multi-agent LLMs with debugging enhances accuracy through runtime feedback, suggesting extensions to our pipeline for handling complex deficiencies.

Our proposal uniquely combines these by iteratively refining human reviews with CRScore-guided LLM enhancements, preserving human value while achieving automated consistency.

---

The field of code review enhancement draws from two primary streams: quality assessment metrics, which provide frameworks for evaluating review effectiveness, and LLM-based generation techniques, which automate or augment the review process. This comprehensive overview integrates foundational and recent contributions, emphasizing how our iterative approach—leveraging CRScore for feedback and LLMs for refinement—addresses limitations in prior works, such as over-reliance on automation or neglect of human intent. We incorporate classifiers for usefulness prediction as complementary tools, selecting the most impactful ones like Random Forest and BERT for their proven performance in identifying actionable comments.

Beginning with quality assessment, early studies laid the groundwork by identifying key attributes of effective reviews. Rahman, Roy, and Kula (2018) in "Predicting Usefulness of Code Review Comments Using Textual Features and Developer Experience" (arXiv:1807.04485) developed the RevHelper dataset and model, utilizing Random Forest to classify comments based on features like readability scores, question ratios, and developer metrics such as authorship commits. Trained on ~10,000 comments from open-source projects, it achieved balanced precision, recall, and F1 of ~66%, highlighting how textual and experiential factors predict if a comment triggers code changes (e.g., within 1-10 lines). This classifier's robustness to non-linear data makes it ideal for baseline usefulness filtering in our pipeline, where we could apply it pre-refinement to prioritize human comments needing enhancement. Similarly, Turzo and Bosu (2023) in "What Makes a Code Review Useful to OpenDev Developers? An Empirical Investigation" (arXiv:2302.11686) analyzed ~5,000 reviews from OpenDev, identifying specificity and coverage as core usefulness drivers via surveys and comment breakdowns. Their empirical insights reveal inconsistencies in human reviews, such as overlooked issues due to fatigue, directly motivating our use of iterative feedback to boost actionability and completeness.

Advancing to metric-driven assessments, Naik, Shetty, Wei, and Le Goues (2024) introduced "CRScore: Grounding Automated Evaluation of Code Review Comments in Code Changes" (arXiv:2405.15204), a neuro-symbolic metric that generates pseudo-references from LLMs (e.g., Magicoder-S-DS-6.7B) and static tools (e.g., PyScent for Python, PMD for Java). It computes comprehensiveness (recall-like coverage of issues), conciseness (precision-like focus), and relevance (harmonic mean), with scores in [0,1] and Spearman correlation ~0.54 to human judgments. Supporting multilingual diffs (Python, Java, JS), CRScore's detailed feedback on mismatches (e.g., missed null values) is central to our methodology, enabling targeted LLM iterations when scores fall below thresholds. Extending this, Naik (2025) in "CRScore++: Reinforcement Learning with Verifiable Tool and AI Feedback for Code Review" (arXiv:2506.00296) incorporates RL for reward-based training, achieving cross-language improvements and suggesting ways to reduce iterations in our loop through optimized models.

Critiques of evaluation paradigms further enrich this area. The 2025 Springer paper "Revisiting the Evaluation of Code Review Comment Generators" (Empirical Software Engineering and Measurement) proposes DeepCRCEval, critiquing BLEU-like metrics and introducing LLM-Reviewer baselines with criteria like actionability, tested on OSS datasets. It exposes benchmark flaws, such as contextual lacks, supporting our grounded approach. Cihan (2025) in "Evaluating Large Language Models for Code Review" (arXiv:2505.20206) compares GPT-4o and Gemini, noting ~68% accuracy in defect detection and advocating human-in-the-loop to counter hallucinations—mirroring our hybrid design. For deep learning classifiers, Bacchelli et al. (2023) in "EvaCRC: Evaluating Code Review Comments" (FSE 2023) used BERT for multi-labeling attributes (e.g., suggestions with F1 0.92), outperforming CNNs on 17,000 annotations and Chromium data, which we select for its contextual superiority in assessing relevance pre-refinement. Complementary reviews include a 2024/2025 MDPI paper "Analysing Quality Metrics and Automated Scoring of Code Reviews" (Software), discussing consistency gaps, and a 2025 ResearchGate "Benchmarks and Metrics for Evaluations of Code Generation: A Critical Review," extending to review metrics for reliability.

Shifting to review generation and enhancement, Li, Lu, Guo et al. (2022) in "Automating Code Review Activities by Large-Scale Pre-Training" (arXiv:2203.09095) presented CodeReviewer, pre-trained on 20,888+ diffs and comments across languages, as a benchmark for generation. Its handling of noisy data aligns with our baselines, where iteration refines similar outputs. Andersson and Nelson (2025) in "Enhancing Code Review at Scale with Generative AI and Knowledge Graphs: An Agentic GraphRAG Framework for Enterprise Code Review" (Blekinge Institute of Technology Thesis) developed ACR for context-aware reviews, reducing load in teams like Ericsson and emphasizing scalability, akin to our GitHub bot vision.

Feedback-driven enhancements are increasingly prominent. The 2024 ACM paper "ClarifyGPT: A Framework for Enhancing LLM-Based Code Generation with Requirement Clarification Questions" (Proceedings of the ACM on Software Engineering) uses LLMs for ambiguity resolution, improving outputs and paralleling our gap-filling via CRScore. A 2024 "AICodeReview: Advancing Code Quality with AI-Enhanced Reviews" (Journal of Software: Evolution and Process) deploys GPT-3.5 in plugins for issue suggestions, but without iteration—our method adds this for preserved human insights. Overviews like the 2025 arXiv "A Survey on Code Generation with LLM-Based Agents" (arXiv:2508.00083) discuss multi-agent paradigms, framing our work innovatively. Finally, a 2025 ResearchGate "Enhancing LLM Code Generation: A Systematic Evaluation of Multi-Agent Collaboration and Runtime Debugging for Improved Accuracy, Reliability, and Latency" evaluates agents with debugging, boosting accuracy and inspiring multi-agent extensions for complex reviews.

This body of work reveals a progression from manual assessments to AI-augmented systems, yet few integrate human preservation with iterative metric-guided refinement. Our proposal fills this by hybridizing CRScore's evaluation with LLM enhancements, drawing datasets like RevHelper, ChromiumConversations, and CodeReviewer for training, and fetching from active repos (e.g., pytorch/pytorch, tensorflow/tensorflow) for real-world diversity. Classifiers like Random Forest and BERT provide optional pre-filters, ensuring usefulness alignment. Overall, while prior efforts automate generation or assess quality in isolation, our approach systematically iterates to produce effective, human-aligned reviews, addressing productivity hindrances in development teams.

**Key Citations:**
- Rahman, M. M., Roy, C. K., & Kula, R. G. (2018). Predicting Usefulness of Code Review Comments Using Textual Features and Developer Experience. arXiv:1807.04485.
- Turzo, A. K., & Bosu, A. (2023). What Makes a Code Review Useful to OpenDev Developers? An Empirical Investigation. arXiv:2302.11686.
- Naik, A., Shetty, S., Wei, H. H., & Le Goues, C. (2024). CRScore: Grounding Automated Evaluation of Code Review Comments in Code Changes. arXiv:2405.15204.
- Naik, A. (2025). CRScore++: Reinforcement Learning with Verifiable Tool and AI Feedback for Code Review. arXiv:2506.00296.
- Revisiting the Evaluation of Code Review Comment Generators. (2025). Springer: Empirical Software Engineering and Measurement.
- Cihan, U. (2025). Evaluating Large Language Models for Code Review. arXiv:2505.20206.
- Bacchelli et al. (2023). EvaCRC: Evaluating Code Review Comments. FSE 2023.
- Analysing Quality Metrics and Automated Scoring of Code Reviews. (2024/2025). MDPI: Software.
- Benchmarks and Metrics for Evaluations of Code Generation: A Critical Review. (2025). ResearchGate.
- Li, Z., Lu, S., Guo, D., et al. (2022). Automating Code Review Activities by Large-Scale Pre-Training. arXiv:2203.09095.
- Andersson, O., & Nelson, I. (2025). Enhancing Code Review at Scale with Generative AI and Knowledge Graphs: An Agentic GraphRAG Framework for Enterprise Code Review. Blekinge Institute of Technology Thesis.
- ClarifyGPT: A Framework for Enhancing LLM-Based Code Generation with Requirement Clarification Questions. (2024). ACM: Proceedings of the ACM on Software Engineering.
- AICodeReview: Advancing Code Quality with AI-Enhanced Reviews. (2024). Journal of Software: Evolution and Process.
- A Survey on Code Generation with LLM-Based Agents. (2025). arXiv:2508.00083.
- Enhancing LLM Code Generation: A Systematic Evaluation of Multi-Agent Collaboration and Runtime Debugging for Improved Accuracy, Reliability, and Latency. (2025). ResearchGate.
