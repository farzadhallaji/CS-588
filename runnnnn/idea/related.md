## Related Work

Research on automating code reviews with large language models (LLMs) intersects software engineering practices, natural language processing evaluation metrics, and advanced fine-tuning techniques. We survey prior efforts in these areas, highlighting gaps that motivate our differentiable adaptation of CRScore for LLM optimization.

### Code Review Automation and LLM Generation
Code reviews have long been automated through static analysis tools like Checkstyle or SonarQube, which detect syntactic issues and code smells but lack contextual natural language feedback. Recent advancements leverage LLMs for generating human-like comments, with models fine-tuned on datasets like CodeReviewer achieving improved relevance through prompt engineering and supervised learning. For instance, multilingual fine-tuning approaches enhance LLM performance across programming languages, demonstrating gains in comment accuracy for non-English codebases. Proprietary codebases pose additional challenges, where fine-tuning on internal data mitigates domain mismatches but raises privacy concerns. RLHF variants have been explored for code generation tasks, such as policy filtration to align outputs with human preferences, yet their application to review comment generation remains limited. Our work extends this by incorporating evaluation signals directly into the training loop, addressing the brittleness of prompt-only methods in diverse review scenarios.

### Evaluation Metrics for LLMs and Code Reviews
Traditional LLM evaluation relies on reference-based metrics like BLEU or ROUGE, which falter in open-ended tasks due to their lexical bias. Semantic alternatives, such as BERTScore, employ embeddings for contextual matching but overlook domain-specific nuances in code reviews. CRScore innovates by grounding assessments in code claims (factual diff-derived statements) and smells, achieving superior human correlation without references. Extensions like CRScore++ incorporate verifiers for enhanced reliability in RLHF pipelines. Broader LLM metrics frameworks emphasize multi-dimensional assessment, including accuracy, efficiency, and ethical considerations like bias detection. While these metrics excel in post-hoc analysis, their discrete nature hinders direct integration into training objectives, a limitation we overcome with CRScore-Loss.

### Fine-Tuning LLMs with Reinforcement Learning and Metrics
Fine-tuning LLMs for specialized tasks often employs supervised methods, but RLHF has proven effective for aligning outputs with human preferences, as seen in foundational tutorials and applications to code generation. In software engineering, RLHF enhances code LLMs by optimizing for debugging robustness or diverse solution generation. QLoRA and prompt augmentation with graphs further improve efficiency in code review contexts. Differentiable metrics, common in NLP for tasks like machine translation, approximate non-differentiable objectives (e.g., BLEU) via sampling or soft alignments. Subjective metrics from textual materials have been proposed for LLM assessment, but not yet adapted for training. Our CRScore-Loss advances this by rendering a domain-specific metric trainable, bridging evaluation and generation in code review pipelines. 
