CRScore is a reference-free quality evaluator for code review comments that compares a reviewer’s text to pseudo-reference claims derived from the code diff; it extracts low-level factual claims (structural changes, code smells) and high-level implications (via an LLM like Magicoder), embeds them plus the comment using Sentence-BERT, and then measures similarity to score comprehensiveness, relevance, and conciseness so that comments can be judged and improved without needing human-written ground truth.

CRScore builds a pseudo-reference for a review by combining symbolic low-level claims (AST/code-smell detectors pull out facts about the diff) with high-level implications generated by a fine-tuned Magicoder LLM; both the pseudo-reference fragments and the reviewer’s sentences get embedded with Sentence-BERT, and pairwise cosine similarities determine coverage.

It then computes three reference-free metrics: ***comprehensiveness*** (what fraction of pseudo-reference items are covered), ***conciseness*** (what fraction of reviewer sentences match something meaningful), and ***relevance*** (the harmonic mean of the two), each ranging [0,1]; reviews below thresholds can be flagged for LLM refinement, so the system can iteratively improve human comments without relying on ground-truth texts.

![image-20251222114416512](/home/ri/.config/Typora/typora-user-images/image-20251222114416512.png)

____

**Run_all.sh Approaches**

- **Data prep + baseline** (run_all.sh (lines 86-93)): freeze a deterministic dev/test split (make_splits.py), build few-shot pairs for proposal v1 (build_fewshot_pairs.py), and score the human-seed baseline via evaluate.py. These ensure consistent inputs and a human comparison point before any automation.
- **Main loop + ablations** (run_all.sh (lines 95-101)): execute the iterative template-editor pipeline (run.py) in several modes—full loop, single edit (k1), single rewrite, no-selection (random CRScore pick), no-evidence, and rewrite-loop—to explore how each constraint affects CRScore-guided refinements.
- **Evaluation sweep** (run_all.sh (lines 103-109)): run evaluate.py on every loop/ablation output to produce per-mode summaries, so you can compare relevance/comprehensiveness/conciseness scores across setups.
- **Proposal v1 Ollama sweep** (run_all.sh (lines 111-144)): if Ollama is available, pull the default plus extra models and run proposal_v1.py with each, then evaluate those outputs—this tests few-shot proposal generation across multiple local LLMs.
- **Threshold-gated refinement sweep** (run_all.sh (lines 146-184)): for each Ollama model and each prompt variant from THRESHOLD_PROMPTS, run threshold_refine.py (threshold 0.6) plus evaluation—this covers how prompt/style + model combinations affect refinement when gated by a quality threshold.
- **Final messaging**: Phase2 human evaluation is intentionally skipped, and results go under $OUT_DIR/$HF_OUT_DIR. Let me know if you want the exact commands used per section or how to customize one of these sweeps.

**FinalProposal Approaches**

- **Reward rerank baseline** (run_final_all.sh (lines 86-107)): generate candidate reviews with the default prompt/temperature, rerank them using weighted CRScore rewards (relevance + penalties on unsupported claims/length/copy), evaluate the selected outputs, and run the robustness suite on that selection.
- **Generation ablations** (run_final_all.sh (lines 109-118)): rerun candidate generation varying temperature, sample count, and prompt variants (default, evidence_grounded, concise, test_heavy) while keeping the base scoring strategy, to study how generation hyperparameters impact downstream ranking.
- **Reward/score ablations** (run_final_all.sh (lines 120-135)): keep the base candidates but change the reranker’s scoring mix (soft vs hard penalties, evidence penalties on/off, tau/temp sweeps, length/copy weights) to see how different reward formulations affect the selected reviews.
- **Preference dumps (DPO prep)** (run_final_all.sh (lines 137-150)): build preference datasets (top/bottom comparisons with and without evidence penalties) from the base candidates, which can later drive preference-based tuning or evaluation.
- **Optional human-study export** (run_final_all.sh (lines 152-161)): if you supply another system output via SYSTEM_B, export aligned human-study pairs to compare the final proposal against that system.
- **Optional DPO LoRA training/inference** (run_final_all.sh (lines 163-196)): when DPO_MODELS is set, train LoRA adapters via DPO on the chosen preference set, then infer with the fine-tuned adapters (generation + evaluation + robustness) to explore preference-trained LLM behavior.
- **Loop robustness check** (run_final_all.sh (lines 198-203)): if the iterative loop baseline exists, run the robustness suite on its outputs for comparison.

_____

# 1. Main loop

- The script sequentially: freezes the dev/test split, builds few-shot pairs, runs the CRScore-guided iterative loop/ablations on the test split (loop, k1, rewrite, no-selection, no-evidence, rewrite-loop), and evaluates each output to produce summaries—so you can compare how each constraint (one-shot, rewrite, random selection, evidence removal) affects relevance/comprehensiveness/conciseness.
- Within the loop, the system keeps the human-written review, scores it against pseudo-reference claims, and repeatedly asks the chosen editor (template by default) for rewrites that cover uncovered claims, trim low-similarity sentences, and satisfy evidence guardrails; this stops when rewards plateau or the iteration budget is exhausted.
- Each ablation just swaps the instruction block the editor receives, so you can trace the effect of edit style, evidence use, and selection policy on the final CRScore outcomes.

**Main loop + ablations prompt text**

- **Base system prompt (always prepended)**:

  `You are a senior code reviewer. Improve the given review comment. Constraints: * Be specific to the diff/claims provided. * Do NOT invent facts, files, or behaviors not supported by the inputs. * Prefer short, high-signal feedback (correctness, edge cases, tests, risks). * Avoid generic advice and style nits unless they directly affect correctness. Output ONLY the revised review text (no explanations, no bullets unless the review itself uses bullets). `

- **loop instruction**:

  `Task: minimally edit the current review. Goals (in priority order): 1. Cover the uncovered pseudo-references using the evidence snippets when available. 2. Remove or rewrite the offending low-similarity sentences. 3. Keep or improve precision: do not add unrelated points. Rules: * If evidence does not support a new statement, do not add it. * If you are unsure, suggest a test instead of asserting behavior. Output only the revised review. `

- **k1 instruction**:

  `Task: produce your best revision in ONE pass. Include the most important uncovered items and delete low-signal/off-topic content. Do not add anything unsupported by evidence or the diff. Output only the revised review. `

- **rewrite instruction**:

  `Task: rewrite the review from scratch for maximum quality. Include the key uncovered pseudo-references and the most important risks/tests. Do not invent facts; ground statements in the diff/evidence. Keep it concise (prefer 1-4 sentences). Output only the revised review. `

- **no-evidence instruction**:

  `Task: improve the review using ONLY the current review + uncovered pseudo-references + the diff context implied by them. Do NOT cite or rely on evidence (assume it is unavailable). Rules: * Do not assert specific code behavior unless it is explicitly stated in the pseudo-references. * Prefer concrete test suggestions over factual claims. Keep it short and specific. Output only the revised review. `

- **no-selection instruction**:

  `Generate an alternative valid revision that is still faithful to the inputs, but phrased differently. Keep the same constraints: no hallucinations, concise, specific, test-focused. `



**Models used in run_all.sh experiments**

- llama3:8b-instruct-q4_0 
- deepseek-coder:6.7b-base-q4_0 
- qwen2.5-coder:7b

**Results:**

- **Base loop (loop_summary.json (line 1))** – The CRScore-guided iterative loop run on the test split scores very low relevance/comprehensiveness/conciseness (all under ~0.23 per language, overall Rel=0.158, Con=0.199, Comp=0.145 over 120 examples), indicating the default minimal-edit strategy rarely hits the pseudo-references without stronger changes.
- **k1 / single-edit (single_edit_summary.json (line 1))** – Running the loop as a single-pass edit yields nearly the same low metrics (overall Rel=0.154, Con=0.192, Comp=0.145), so the one-shot constraint doesn’t materially change coverage either.
- **no-selection and no-evidence (no_selection_summary.json (line 1), no_evidence_summary.json (line 1))** – Swapping to random selection or forbidding evidence also keeps the scores stagnant (Rel≈0.158, Con≈0.199, Comp≈0.145), showing those safeguards are critical: once you drop reward-ranking or grounding, coverage collapses.
- **single_rewrite and rewrite_loop (single_rewrite_summary.json (line 1), rewrite_loop_summary.json (line 1))** – Allowing full rewrites dramatically improves quality: single rewrites hit overall Rel≈0.493/Con≈0.505/Comp≈0.504, and the full rewrite loop (multiple iterations rebuilding the review) pushes even higher (Rel≈0.521, Con≈0.549, Comp≈0.511), proving that aggressive editing is necessary to satisfy the pseudo-reference claims from this split.

_____

# notes

- run_all.sh first ensures the dataset split and few-shot pairs are deterministic, then runs the iterative refinement pipeline on the test split in several operating modes; each mode runs the CRScore-guided loop, saves the JSONL output, and immediately evaluates it, so you get a summary score card for every variation.