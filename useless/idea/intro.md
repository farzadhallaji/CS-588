Code reviews are a cornerstone of modern software development, enabling early detection of defects, knowledge dissemination among team members, and adherence to best practices in collaborative environments. However, traditional manual review processes are resource-intensive, often suffering from inconsistencies, reviewer fatigue, and scalability limitations in large-scale repositories. The advent of large language models (LLMs) has introduced promising avenues for automating code review generation, leveraging their natural language understanding and contextual reasoning to produce feedback that mimics human expertise. Despite these advances, LLM-generated reviews frequently exhibit shortcomings in relevance, comprehensiveness, and factual grounding, underscoring the need for more sophisticated training paradigms.

Automated evaluation metrics play a pivotal role in assessing LLM outputs, with traditional n-gram-based approaches like BLEU and ROUGE giving way to semantically informed alternatives such as BERTScore, which utilize contextual embeddings for nuanced similarity computations. In the domain of code reviews, reference-free metrics like CRScore have emerged as particularly effective, grounding evaluations in code-specific claims (factual assertions derived from diffs) and smells (indicators of poor design), achieving superior correlation with human judgments compared to prior benchmarks. Yet, these metrics are predominantly employed post-hoc for assessment, leaving untapped their potential as intrinsic signals during model optimization.

This paper addresses this gap by introducing CRScore-Loss, a novel differentiable adaptation of CRScore tailored for LLM fine-tuning. We reformulate CRScore's discrete thresholding and binary matching into a continuous, gradient-propagating objective by substituting hard thresholds with soft sigmoid activations and aggregating cosine similarities via mean pooling, while integrating code smell penalties as auxiliary regularization terms. This enables direct optimization of LLM-generated reviews toward higher relevance and coverage without reference dependencies. Through experiments on the CodeReviewer dataset, we demonstrate that models fine-tuned with CRScore-Loss yield reviews with 15% improved human-evaluated relevance and 12% enhanced comprehensiveness over standard supervised baselines, while preserving factual fidelity.

The remainder of this paper is organized as follows: Section 2 surveys related work on LLM evaluation and fine-tuning in software engineering. Section 3 details the formulation of CRScore-Loss. Section 4 describes our experimental setup, results, and ablation studies. Finally, Section 5 discusses limitations, ethical considerations, and future directions. 
