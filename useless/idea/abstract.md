Automated evaluation metrics for code review comments, such as CRScore, have demonstrated strong correlation with human judgments by grounding assessments in code claims and smells without relying on references. However, their potential as training objectives for large language models (LLMs) remains unexplored. In this paper, we propose CRScore-Loss, a differentiable adaptation of CRScore that transforms its discrete thresholding and matching mechanisms into a continuous, gradient-friendly loss function. By approximating binary similarities with soft cosine averages and incorporating code smell signals as auxiliary rewards, we enable end-to-end fine-tuning of LLMs for generating high-quality code reviews. Experiments on the CodeReviewer dataset show that models fine-tuned with CRScore-Loss outperform baselines in human-evaluated relevance (up 15%) and comprehensiveness (up 12%), while maintaining factual accuracy. Our approach bridges evaluation and generation in software engineering, paving the way for more aligned LLM-assisted code reviews. 
